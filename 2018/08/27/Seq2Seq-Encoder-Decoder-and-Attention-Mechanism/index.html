<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Monaco:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Seq2Seq,Attention," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="对于循环神经网络，它的输入是一段不定长的序列，输出却是定长的。然而，很多问题的输出也是不定长的序列，比如机器翻译。当输入输出都是不定长序列时，我们可以使用编码器-解码器 (encoder-decoder) 来对其进行建模。">
<meta name="keywords" content="Seq2Seq,Attention">
<meta property="og:type" content="article">
<meta property="og:title" content="Seq2Seq (Encoder-Decoder) and Attention Mechanism">
<meta property="og:url" content="http://yanshengjia.com/2018/08/27/Seq2Seq-Encoder-Decoder-and-Attention-Mechanism/index.html">
<meta property="og:site_name" content="WildPointer">
<meta property="og:description" content="对于循环神经网络，它的输入是一段不定长的序列，输出却是定长的。然而，很多问题的输出也是不定长的序列，比如机器翻译。当输入输出都是不定长序列时，我们可以使用编码器-解码器 (encoder-decoder) 来对其进行建模。">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/nmt-model-fast.gif">
<meta property="og:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/encoder_decoder.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/rnn_loop.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/unrolled_rnn.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/sequences.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/seq2seq.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/s2s_prob1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/s2s_prob2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/beam_search.png">
<meta property="og:updated_time" content="2018-09-05T03:22:24.326Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Seq2Seq (Encoder-Decoder) and Attention Mechanism">
<meta name="twitter:description" content="对于循环神经网络，它的输入是一段不定长的序列，输出却是定长的。然而，很多问题的输出也是不定长的序列，比如机器翻译。当输入输出都是不定长序列时，我们可以使用编码器-解码器 (encoder-decoder) 来对其进行建模。">
<meta name="twitter:image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/nmt-model-fast.gif">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide","offset":12,"offset_float":0,"b2t":true,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'K1YPL7B54D',
      apiKey: '7f22077ed5b2dfbd70cc761b8cf48112',
      indexName: 'wildpointer',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yanshengjia.com/2018/08/27/Seq2Seq-Encoder-Decoder-and-Attention-Mechanism/"/>





  <title> Seq2Seq (Encoder-Decoder) and Attention Mechanism | WildPointer </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">WildPointer</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Welcome to my world!</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Arc
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Cat
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tag
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-star"></i> <br />
            
            Proj
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tech">
          <a href="/tech" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-code"></i> <br />
            
            Tech
          </a>
        </li>
      
        
        <li class="menu-item menu-item-donate">
          <a href="/donate" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-usd"></i> <br />
            
            Donate
          </a>
        </li>
      
        
        <li class="menu-item menu-item-me">
          <a href="/me" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            Me
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yanshengjia.com/2018/08/27/Seq2Seq-Encoder-Decoder-and-Attention-Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Shengjia Yan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://raw.githubusercontent.com/yanshengjia/photo/master/skydive0.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WildPointer">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Seq2Seq (Encoder-Decoder) and Attention Mechanism
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-27T11:51:25+08:00">
                2018-08-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/08/27/Seq2Seq-Encoder-Decoder-and-Attention-Mechanism/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/08/27/Seq2Seq-Encoder-Decoder-and-Attention-Mechanism/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>对于循环神经网络，它的输入是一段不定长的序列，输出却是定长的。然而，很多问题的输出也是不定长的序列，比如机器翻译。当输入输出都是不定长序列时，我们可以使用编码器-解码器 (encoder-decoder) 来对其进行建模。</p>
<a id="more"></a>
<hr>
<h1 id="介绍-Introduction"><a href="#介绍-Introduction" class="headerlink" title="介绍 Introduction"></a>介绍 Introduction</h1><p>在很多 NLP 应用中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是一段不定长的中文文本序列，输出可以是一段不定长的英语文本序列，例如</p>
<p>中文输入：”知”, “识”, “就”, “是”, “力”, “量”</p>
<p>英语输出：”Knowledge”, “is”, “power”</p>
<p><img src="https://raw.githubusercontent.com/yanshengjia/photo/master/nmt-model-fast.gif" alt="NMT"></p>
<p>当输入输出都是不定长序列时，我们可以使用编码器-解码器 (encoder-decoder) 或者 seq2seq。它们分别基于2014年的两个工作：</p>
<ul>
<li>Cho et al., Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</li>
<li>Sutskever et al., Sequence to Sequence Learning with Neural Networks</li>
</ul>
<p>Cho 是 Yoshua Bengio 团队的，论文中 RNN Cell 使用的是 GRU，这也是 GRU 首次被提出！Sutskever 是谷歌团队的，论文中 RNN Cell 用的是 LSTM。RNN Cell 的不同是这两篇论文的主要区别。</p>
<p>这两个工作几乎是同时提出，本质上都用到了2个循环神经网络 (RNN)，分别叫做编码器 (Encoder) 和解码器 (Decoder)。编码器对应于输入序列，将输入序列的信息编码到上下文变量 (context variable) 中；解码器对应输出序列，将上下文变量解码成输出序列。</p>
<p><img src="https://raw.githubusercontent.com/yanshengjia/photo/master/encoder_decoder.png" alt="encoder-decoder"></p>
<p>在普通的编码器-解码器模型中，有一个很大的局限性。那就是上下文变量对于 Decoding 阶段每个时间步都是一样的，这可能是模型性能的一个瓶颈。我们希望不同时间步的解码能够依赖于与之更相关的上下文信息，换句话说，Decoding 往往并不需要整个输入序列的信息，要有所侧重。于是，Bengio 团队的 Bahdanau 在 2014年首次在编码器-解码器模型中引入了注意力机制 (Attention Mechanism)。</p>
<ul>
<li>Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate</li>
</ul>
<p>在介绍编码器-解码器的设计之前，先简单介绍一下循环神经网络 (下文中用 RNN 指代)。</p>
<hr>
<h1 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络 RNN"></a>循环神经网络 RNN</h1><p>循环神经网络 (Recurrent Neural Network) 是一类用于处理序列数据的神经网络。它不是刚性地记住所有固定长度的序列，而是通过隐藏状态来储存前面时间的信息。在 RNN 中，当前时刻的隐层节点的输出会作为下一时刻的隐层节点的输入，使得她可以保留之前的信息。</p>
<p><img src="https://raw.githubusercontent.com/yanshengjia/photo/master/rnn_loop.png" alt="rnn_loop"></p>
<p>这个循环的结构让 RNN 看起来有些难以理解。但是，RNN 其实与普通的神经网络有不少相似之处。我们可以按时间维度将其展开，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/yanshengjia/photo/master/unrolled_rnn.png" alt="rnn_loop"></p>
<p>RNN 的链状结构让她看起来就像是为序列数据而生。</p>
<p>原始的 BP 神经网络 (或者原始 CNN) 最大的局限之处在于：它们将固定大小的向量作为输入 (比如一张图片)，然后输出一个固定大小的向量 (比如不同分类的概率)。而且：这些模型按照固定的计算步骤来 (比如模型中 layer 的数量) 实现这样的映射。RNN 的特别之处在于，她允许我们控制向量序列的长度：输入序列、输出序列、或者输入输出序列。通过下面几个例子可以具体理解这点：</p>
<p><img src="https://raw.githubusercontent.com/yanshengjia/photo/master/sequences.png" alt="sequences"></p>
<p>每一个矩形是一个向量，箭头则表示函数 (比如矩阵乘法)。输入向量用红色标出，输出向量用蓝色标出，绿色的矩形是 RNN 的状态。从左到右：</p>
<ul>
<li>1-&gt;1: 没有使用 RNN 的原始模型，从固定大小的输入得到固定大小输出 (比如图像分类)</li>
<li>1-&gt;N: 序列输出 (比如图片字幕，输入一张图片输出一段文字序列)</li>
<li>N-&gt;1: 序列输入 (比如情感分析，输入一段文字然后将它分类成积极或者消极情感)</li>
<li>N-&gt;M: 序列输入和序列输出 (比如机器翻译：一个 RNN 读取一条中文语句然后将它以英语形式输出)</li>
<li>N-&gt;N: 同步序列输入输出 (比如视频分类，对视频中每一帧打标签)。</li>
</ul>
<p>我们注意到在每一个案例中，都没有对序列长度进行预先特定约束，因为递归变换 (绿色部分) 是固定的，而且我们可以根据需要多次使用。</p>
<p>在近些年，人们利用 RNN 不可思议地解决了各种各样的问题：语音识别，语言模型，翻译，图像描述，等等。关于 RNN 在这些方面取得的惊人成功，我们可以看 Andrej Karpathy 的博文：<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a>.</p>
<p>RNN 之所以能够取得这样的成功，主要是因为 LSTM / GRU 的使用。他们都是带门控 (gate) 的 RNN 单元，用于解决长期依赖 (long-term dependencies) 的问题，对于很多任务，比普通的 RNN 单元效果要好很多！基本上现在所使用的循环神经网络用的都是 LSTM / GRU 单元。</p>
<p>介绍完了 RNN，让我们回到这篇文章的主题，编码器-解码器。编码器和解码器都是 RNN，上下文变量为二者搭建起了信息传递的桥梁，编码器将输入序列的信息编码到上下文变量中，解码器将上下文变量中的信息解码生成输出序列。编码器-解码器的设计是多种多样的，需要根据具体问题具体分析。</p>
<hr>
<h1 id="编码器-解码器-Encoder-Decoder"><a href="#编码器-解码器-Encoder-Decoder" class="headerlink" title="编码器-解码器 Encoder-Decoder"></a>编码器-解码器 Encoder-Decoder</h1><p>在很多应用中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是一段不定长的英语文本序列，输出可以是一段不定长的法语文本序列，例如</p>
<blockquote>
<p>英语输入：“They”、“are”、“watching”、“.”</p>
<p>法语输出：“Ils”、“regardent”、“.”</p>
</blockquote>
<p>当输入输出都是不定长序列时，我们可以使用编码器—解码器 (encoder-decoder) 或者 seq2seq 模型。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器对应输入序列，解码器对应输出序列。</p>
<p>下图描述了使用编码器—解码器将上述英语句子翻译成法语句子的一种方法。在训练数据集中，我们可以在每个句子后附上特殊符号 “&lt;eos&gt;” (end of sequence) 表示序列的终止。编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号 “&lt;eos&gt;”。下图使用了编码器在最终时间步的隐藏状态作为输入句子的编码信息。解码器在各个时间步中使用输入句子的编码信息和上个时间步的输出以及隐藏状态作为输入。</p>
<p>我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号 “&lt;eos&gt;”。<br>需要注意的是，解码器在最初时间步的输入用到了一个表示序列开始的特殊符号 “&lt;bos&gt;” (beginning of sequence) 。</p>
<p><img src="https://raw.githubusercontent.com/yanshengjia/photo/master/seq2seq.png" alt="使用编码器—解码器将句子由英语翻译成法语。编码器和解码器分别为循环神经网络。"></p>
<p>接下来我们介绍编码器和解码器的定义。</p>
<h2 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器 Encoder"></a>编码器 Encoder</h2><p>编码器的作用是把一个不定长的输入序列变换成一个定长的上下文变量 $\boldsymbol{c}$，并在该上下文变量中编码输入序列信息。常用的编码器是循环神经网络。</p>
<p>现在来看一下上一节的示意图中的编码器部分，”They are watching . &lt;eos&gt;” 是输入序列，由右箭头连起来的蓝色方块序列就是 RNN，蓝色方块是 RNN 的隐层节点 (包含了当前时间步的 hidden state)。</p>
<p>让我们考虑批量大小为1的时序数据样本。假设输入序列是 <script type="math/tex">x_1, \ldots, x_T</script>，例如 <script type="math/tex">x_i</script> 是输入句子中的第 $i$ 个词。在每一个时间步，输入单词 <script type="math/tex">x_t</script> 的词向量 <script type="math/tex">\boldsymbol{x}_t</script> 会做一次矩阵投影 (matrix projection) 加上偏置项，然后用激活函数激活，得到当前时间步的隐藏状态 <script type="math/tex">\boldsymbol{h}_t</script>。我们可以用下面的公式表达编码器中时间步 $t$ 的隐藏状态 (假设使用 sigmoid 激活函数)：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_t = \sigma(W\boldsymbol{x}_t + b)</script><p>在时间步 $t$，循环神经网络将输入 <script type="math/tex">x_t</script> 的特征向量 <script type="math/tex">\boldsymbol{x}_t</script> 和上个时间步的隐藏状态 <script type="math/tex">\boldsymbol{h}_{t-1}</script> 变换为当前时间步的隐藏状态 <script type="math/tex">\boldsymbol{h}_t</script> 。我们可以用函数 <script type="math/tex">f</script> 表达循环神经网络隐藏层的变换：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_t = f(\boldsymbol{x}_t, \boldsymbol{h}_{t-1})</script><p>这里特征向量 $\boldsymbol{x}_t$ 既可以是需要学习的词向量，也可以是预训练的词向量。如果使用的是预训练的词向量，我们可以对输入 <script type="math/tex">x_t</script> 的特征向量 <script type="math/tex">\boldsymbol{x}_t</script> 进行 one-hot 编码，直接从预训练的词向量矩阵 <script type="math/tex">\boldsymbol{E}</script> 读出相应编号的行向量。这也是为什么 Embedding layer 被称为 lookup table 的原因；如果想去学习特征向量 $\boldsymbol{x}_t$ 的词向量，定好词向量矩阵 <script type="math/tex">\boldsymbol{E}</script> 的维度，把它当做模型参数去学。</p>
<p>假设输入序列的总时间步数为 $T$ 。接下来编码器通过自定义函数 $q$ 将各个时间步的隐藏状态变换为上下文变量</p>
<script type="math/tex; mode=display">
\boldsymbol{c} =  q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T)</script><p>例如，当选择 $q(\boldsymbol{h}_1, \ldots, \boldsymbol{h}_T) = \boldsymbol{h}_T$ 时，上下文变量是输入序列最终时间步的隐藏状态 $\boldsymbol{h}_T$ 。</p>
<p>上下文变量 $\boldsymbol{c}$ 可以认为是输入序列的语义表达，它总结了输入序列的信息，但它不对输入序列直接编码，而是对输入序列对应的隐含状态编码。</p>
<p>上下文变量 <script type="math/tex">\boldsymbol{c}</script> 编码的方式是多种多样的，直接选择最终时间步的隐藏状态是最简单粗暴的，这样的 Encoder 中其实使用了一个正向的 RNN，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们可以用下面的公式来表达正向的 RNN：</p>
<script type="math/tex; mode=display">
\vec{\boldsymbol{h}_t} = f(\boldsymbol{x}_t, \vec{\boldsymbol{h}_{t-1}})</script><p>除了正向的 RNN，Encoder 中还可以使用反向的 RNN，每个时间步的隐藏状态只取决于该时间步及之后的输入子序列，上下文编码 <script type="math/tex">\boldsymbol{c}</script> 取第一个时间步的隐藏状态。形象地说，假如我们认为向右是正向的话，正向的 RNN 是从左向右扫一遍，反向的 RNN 是从右向左扫一遍。我们可以用下面的公式来表达反向的 RNN：</p>
<script type="math/tex; mode=display">
\overleftarrow{\boldsymbol{h}_t} = f(\boldsymbol{x}_t, \overleftarrow{\boldsymbol{h}_{t+1}})</script><p>我们也可以使用双向循环神经网络构造编码器。这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列 (包括当前时间步的输入) ，并编码了整个序列的信息。</p>
<p>总之，Encoder 的设计是多种多样的。</p>
<h2 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器 Decoder"></a>解码器 Decoder</h2><p>刚刚已经介绍，编码器输出的上下文变量 $\boldsymbol{c}$ 编码了整个输入序列 <script type="math/tex">x_1, \ldots, x_T</script> 的信息。给定训练样本中的输出序列 <script type="math/tex">y_1, y_2, \ldots, y_{T'}</script> ，对每个时间步 $t’$ ，解码器输出 <script type="math/tex">y_{t'}</script> 的条件概率将基于之前的输出序列 <script type="math/tex">y_1, \ldots, y_{t'-1}</script> 和上下文变量 $\boldsymbol{c}$ ，即 <script type="math/tex">\mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})</script> 。</p>
<p>为此，我们可以使用另一个循环神经网络作为解码器。</p>
<p>在输出序列的时间步 $t^\prime$ ，解码器将上一时间步的输出 <script type="math/tex">y_{t^\prime-1}</script> 以及上下文变量 <script type="math/tex">\boldsymbol{c}</script> 作为输入，并将它们与上一时间步的隐藏状态 <script type="math/tex">\boldsymbol{s}_{t^\prime-1}</script> 变换为当前时间步的隐藏状态 <script type="math/tex">\boldsymbol{s}_{t^\prime}</script> 。因此，我们可以用函数 <script type="math/tex">g</script> 表达解码器隐藏层的变换：</p>
<script type="math/tex; mode=display">
\boldsymbol{s}_{t^\prime} = g(y_{t^\prime-1}, \boldsymbol{c}, \boldsymbol{s}_{t^\prime-1})</script><p>有了解码器的隐藏状态后，我们可以使用自定义的输出层和 softmax 运算来计算 <script type="math/tex">\mathbb{P}(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \boldsymbol{c})</script> ，例如基于当前时间步的解码器隐藏状态 <script type="math/tex">\boldsymbol{s}_{t^\prime}</script>、上一时间步的输出 <script type="math/tex">y_{t^\prime-1}</script> 以及上下文变量 <script type="math/tex">\boldsymbol{c}</script> 来计算当前时间步输出 <script type="math/tex">y_{t^\prime}</script> 的概率分布。</p>
<h2 id="模型训练-Model-Training"><a href="#模型训练-Model-Training" class="headerlink" title="模型训练 Model Training"></a>模型训练 Model Training</h2><p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{P}(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T)
&= \prod_{t'=1}^{T'} \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, x_1, \ldots, x_T)\\
&= \prod_{t'=1}^{T'} \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})
\end{aligned}</script><p>并得到该输出序列的损失</p>
<script type="math/tex; mode=display">
\log\mathbb{P}(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T) = -\sum_{t'=1}^{T'} \log \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})</script><p>在模型训练中，我们通过最小化这个损失函数来得到模型参数。</p>
<h3 id="训练技巧-Tricks"><a href="#训练技巧-Tricks" class="headerlink" title="训练技巧 Tricks"></a>训练技巧 Tricks</h3><h2 id="模型预测-Model-Prediction"><a href="#模型预测-Model-Prediction" class="headerlink" title="模型预测 Model Prediction"></a>模型预测 Model Prediction</h2><p>上一节介绍了如何训练输入输出均为不定长序列的编码器—解码器，这一节我们介绍如何使用编码器—解码器来预测不定长的序列。</p>
<p>上一节里已经提到，在准备训练数据集时，我们通常会在样本的输入序列和输出序列后面分别附上一个特殊符号 “&lt;eos&gt;” 表示序列的终止。我们在接下来的讨论中也将沿用上一节的数学符号。为了便于讨论，假设解码器的输出是一段文本序列。设输出文本词典 $\mathcal{Y}$ (包含特殊符号“&lt;eos&gt;”) 的大小为 $\left|\mathcal{Y}\right|$ ，输出序列的最大长度为 $T’$ 。所有可能的输出序列一共有 $\mathcal{O}(\left|\mathcal{Y}\right|^{T’})$ 种。这些输出序列中所有特殊符号“&lt;eos&gt;”后面的子序列将被舍弃。</p>
<h3 id="穷举搜索-Exhaustive-Search"><a href="#穷举搜索-Exhaustive-Search" class="headerlink" title="穷举搜索 Exhaustive Search"></a>穷举搜索 Exhaustive Search</h3><p>我们在上一节描述解码器时提到，输出序列基于输入序列的条件概率是 <script type="math/tex">\prod_{t'=1}^{T'} \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})</script> 。为了搜索该条件概率最大的输出序列，一种方法是穷举所有可能输出序列的条件概率，并输出条件概率最大的序列。我们将该序列称为最优序列，并将这种搜索方法称为穷举搜索 (exhaustive search)。</p>
<p>虽然穷举搜索可以得到最优序列，但它的计算开销 $\mathcal{O}(\left|\mathcal{Y}\right|^{T’})$ 很容易过大。例如，当 $|\mathcal{Y}|=10000$ 且 $T’=10$ 时，我们将评估 $10000^{10} = 10^{40}$ 个序列：这几乎不可能完成。</p>
<h3 id="贪婪搜索-Greedy-Search"><a href="#贪婪搜索-Greedy-Search" class="headerlink" title="贪婪搜索 Greedy Search"></a>贪婪搜索 Greedy Search</h3><p>我们还可以使用贪婪搜索 (greedy search) 。也就是说，对于输出序列任一时间步 $t’$，从 $|\mathcal{Y}|$ 个词中搜索出输出词</p>
<script type="math/tex; mode=display">
y_{t'} = \text{argmax}_{y_{t'} \in \mathcal{Y}} \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})</script><p>且一旦搜索出 “&lt;eos&gt;” 符号即完成输出序列。贪婪搜索的计算开销是 $\mathcal{O}(\left|\mathcal{Y}\right|T’)$。它比起穷举搜索的计算开销显著下降。例如，当 $|\mathcal{Y}|=10000$ 且 $T’=10$ 时，我们只需评估 $10000\times10=1\times10^5$ 个序列。</p>
<p>下面我们来看一个例子。假设输出词典里面有 “A”、“B”、“C”和 “&lt;eos&gt;” 这四个词。下图中每个时间步下的四个数字分别代表了该时间步生成 “A”、“B”、“C”和 “&lt;eos&gt;” 这四个词的条件概率。在每个时间步，贪婪搜索选取生成条件概率最大的词。因此，将生成序列 “ABC&lt;eos&gt;” 。该输出序列的条件概率是 $0.5\times0.4\times0.4\times0.6 = 0.048$。</p>
<p><img src="https://raw.githubusercontent.com/yanshengjia/photo/master/s2s_prob1.png" alt="每个时间步下的四个数字分别代表了该时间步生成“A”、“B”、“C”和“&lt;eos&gt;”这四个词的条件概率。在每个时间步，贪婪搜索选取生成条件概率最大的词。"></p>
<p>正如绝大部分贪婪算法不能保证最优解一样，贪婪搜索也无法保证找出条件概率最大的最优序列。下图演示了这样的一个例子。与上图中不同，下图在时间步2中选取了条件概率第二大的 “C” 。由于时间步3所基于的时间步1和2的输出子序列由上图中的 “AB” 变为了下图中的 “AC”，下图中时间步3生成各个词的条件概率发生了变化。我们选取条件概率最大的 “B”。此时时间步4所基于的前三个时间步的输出子序列为 “ACB”，与上图中的 “ABC” 不同。因此下图时间步4生成各个词的条件概率也与上图中的不同。我们发现，此时的输出序列 “ACB&lt;eos&gt;” 的条件概率是$0.5\times0.3\times0.6\times0.6=0.054$，大于贪婪搜索得到的输出序列的条件概率。因此，贪婪搜索得到的输出序列 “ABC&lt;eos&gt;” 并非最优序列。</p>
<p><img src="https://raw.githubusercontent.com/yanshengjia/photo/master/s2s_prob2.png" alt="每个时间步下的四个数字分别代表了该时间步生成“A”、“B”、“C”和“&lt;eos&gt;”这四个词的条件概率。在时间步2选取条件概率第二大的“C”。"></p>
<h3 id="束搜索-Beam-Search"><a href="#束搜索-Beam-Search" class="headerlink" title="束搜索 Beam Search"></a>束搜索 Beam Search</h3><p>束搜索 (beam search) 是比贪婪搜索更加广义的搜索算法。它有一个束宽 (beam size) 超参数。我们将它设为 $k$。在时间步1时，选取当前时间步生成条件概率最大的 $k$ 个词，分别组成 $k$ 个候选输出序列的首词。在之后的每个时间步，基于上个时间步的 $k$ 个候选输出序列，从 $k\left|\mathcal{Y}\right|$ 个可能的输出序列中选取生成条件概率最大的 $k$ 个，作为该时间步的候选输出序列。<br>最终，我们在各个时间步的候选输出序列中筛选出包含特殊符号 “&lt;eos&gt;” 的序列，并将它们中所有特殊符号 “&lt;eos&gt;” 后面的子序列舍弃，得到最终候选输出序列。在这些最终候选输出序列中，取以下分数最高的序列作为输出序列：</p>
<script type="math/tex; mode=display">
\frac{1}{L^\alpha} \log \mathbb{P}(y_1, \ldots, y_{L}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})</script><p>其中 $L$ 为最终候选序列长度，$\alpha$ 一般可选为0.75。分母上的 $L^\alpha$ 是为了惩罚较长序列在以上分数中较多的对数相加项。分析可得，束搜索的计算开销为 $\mathcal{O}(k\left|\mathcal{Y}\right|T’)$。这介于穷举搜索和贪婪搜索的计算开销之间。</p>
<p><img src="https://raw.githubusercontent.com/yanshengjia/photo/master/beam_search.png" alt="束搜索的过程。束宽为2，输出序列最大长度为3。候选输出序列有$A$、$C$、$AB$、$CE$、$ABD$和$CED$。"></p>
<p>上图通过一个例子演示了束搜索的过程。假设输出序列的词典中只包含五个元素：$\mathcal{Y} = {A, B, C, D, E}$，且其中一个为特殊符号 “&lt;eos&gt;”。设束搜索的束宽等于2，输出序列最大长度为3。在输出序列的时间步1时，假设条件概率 $\mathbb{P}(y_1 \mid \boldsymbol{c})$ 最大的两个词为 $A$ 和 $C$。我们在时间步2时将对所有的 $y_2 \in \mathcal{Y}$ 都分别计算 $\mathbb{P}(y_2 \mid A, \boldsymbol{c})$ 和 $\mathbb{P}(y_2 \mid C, \boldsymbol{c})$，并从计算出的10个条件概率中取最大的两个：假设为 $\mathbb{P}(B \mid A, \boldsymbol{c})$ 和$\mathbb{P}(E \mid C, \boldsymbol{c})$。那么，我们在时间步3时将对所有的 $y_3 \in \mathcal{Y}$ 都分别计算 $\mathbb{P}(y_3 \mid A, B, \boldsymbol{c})$ 和 $\mathbb{P}(y_3 \mid C, E, \boldsymbol{c})$，并从计算出的10个条件概率中取最大的两个：假设为 $\mathbb{P}(D \mid A, B, \boldsymbol{c})$ 和 $\mathbb{P}(D \mid C, E, \boldsymbol{c})$。接下来，我们可以在6个候选输出序列：$A$、$C$、$AB$、$CE$、$ABD$和$CED$中筛选出包含特殊符号“&lt;eos&gt;”的序列，并将它们中所有特殊符号“&lt;eos&gt;”后面的子序列舍弃，得到最终候选输出序列。我们可以在最终候选输出序列中取分数最高的序列作为输出序列。</p>
<p>贪婪搜索可看作是束宽为1的束搜索。束搜索通过更灵活的束宽 $k$ 来权衡计算开销和搜索质量。</p>
<hr>
<h1 id="注意力机制-Attention-Mechanism"><a href="#注意力机制-Attention-Mechanism" class="headerlink" title="注意力机制 Attention Mechanism"></a>注意力机制 Attention Mechanism</h1><p>在”编码器—解码器”一节里的解码器设计中，输出序列的各个时间步使用了相同的上下文变量。如果解码器的不同时间步可以使用不同的上下文变量呢？这样做有什么好处？</p>
<h2 id="动机-Motivation"><a href="#动机-Motivation" class="headerlink" title="动机 Motivation"></a>动机 Motivation</h2><p>以英语-法语翻译为例，给定一对英语输入序列 “They”、“are”、“watching”、“.” 和法语输出序列 “Ils”、“regardent”、“.”。解码器可以在输出序列的时间步1使用更集中编码了 “They”、“are” 信息的上下文变量来生成 “Ils”，在时间步2使用更集中编码了 “watching” 信息的上下文变量来生成“regardent”，在时间步3使用更集中编码了 “.” 信息的上下文变量来生成 “.”。这看上去就像是在解码器的每一时间步对输入序列中不同时间步编码的信息分配不同的注意力。这也是注意力机制的由来。它最早由 Bahanau 等人提出。</p>
<h2 id="设计-Design"><a href="#设计-Design" class="headerlink" title="设计 Design"></a>设计 Design</h2><p>本节沿用”编码器—解码器”一节里的数学符号。</p>
<p>我们对”编码器—解码器”一节里的解码器稍作修改。在时间步 $t’​$，设解码器的上下文变量为 <script type="math/tex">\boldsymbol{c}_{t'}​</script>，输出 <script type="math/tex">y_{t'}​</script> 的特征向量为 <script type="math/tex">\boldsymbol{y}_{t'} ​</script>。</p>
<p>和输入的特征向量一样，这里每个输出的特征向量也是模型参数。解码器在时间步 $t’$ 的隐藏状态</p>
<script type="math/tex; mode=display">
\boldsymbol{s}_{t'} = g(\boldsymbol{y}_{t'-1}, \boldsymbol{c}_{t'}, \boldsymbol{s}_{t'-1})</script><p>令编码器在时间步$t$的隐藏状态为 $\boldsymbol{h}_t$，且总时间步数为 $T$。解码器在时间步 $t’$ 的背景变量为</p>
<script type="math/tex; mode=display">
\boldsymbol{c}_{t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t</script><p>其中 $\alpha_{t’ t}$ 是权值。也就是说，给定解码器的当前时间步 $t’$，我们需要对编码器中不同时间步 $t$ 的隐藏状态求加权平均。这里的权值也称注意力权重。它的计算公式是</p>
<script type="math/tex; mode=display">
\alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) }</script><p>其中 $e_{t’ t} \in \mathbb{R}$ 的计算为</p>
<script type="math/tex; mode=display">
e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t)</script><p>上式中的函数 $a$ 有多种设计方法。Bahanau 等使用了多层感知机：</p>
<script type="math/tex; mode=display">
e_{t' t} = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_h \boldsymbol{h}_t)</script><p>其中 $\boldsymbol{v}$、$\boldsymbol{W}_s$、$\boldsymbol{W}_h$ 以及编码器与解码器中的各个权重和偏差都是模型参数。</p>
<p>Bahanau 等在编码器和解码器中分别使用了门控循环单元 GRU。在解码器中，我们需要对门控循环单元的设计稍作修改。解码器在 $t’ $ 时间步的隐藏状态为</p>
<script type="math/tex; mode=display">
\boldsymbol{s}_{t'} = \boldsymbol{z}_{t'} \odot \boldsymbol{s}_{t'-1}  + (1 - \boldsymbol{z}_{t'}) \odot \tilde{\boldsymbol{s}}_{t'}</script><p>其中的重置门、更新门和候选隐含状态分别为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{r}_{t'} &= \sigma(\boldsymbol{W}_{yr} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sr} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cr} \boldsymbol{c}_{t'} + \boldsymbol{b}_r),\\
\boldsymbol{z}_{t'} &= \sigma(\boldsymbol{W}_{yz} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{sz} \boldsymbol{s}_{t' - 1} + \boldsymbol{W}_{cz} \boldsymbol{c}_{t'} + \boldsymbol{b}_z),\\
\tilde{\boldsymbol{s}}_{t'} &= \text{tanh}(\boldsymbol{W}_{ys} \boldsymbol{y}_{t'-1} + \boldsymbol{W}_{ss} (\boldsymbol{s}_{t' - 1} \odot \boldsymbol{r}_{t'}) + \boldsymbol{W}_{cs} \boldsymbol{c}_{t'} + \boldsymbol{b}_s).
\end{aligned}</script><hr>
<h1 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h1><p>1.除了机器翻译 (Neural Machine Translation)，你还能想到 seq2seq 的哪些应用？</p>
<p>答：</p>
<ul>
<li>文本摘要 Text Summarization</li>
<li>图像描述 Image Captioning</li>
<li>语法纠错 Grammar Error Correction</li>
<li>对话生成</li>
<li>自动邮件回复</li>
<li>诗词生成、歌词生成</li>
<li>生成 git commit message</li>
<li>生成代码补全</li>
</ul>
<p>Seq2Seq 主要可以用于自然语言生成。</p>
<p>自然语言生成 (NLG) 是一个非常有意思的研究领域，简单地说就是解决一个条件概率 P(output | context) 的建模问题，根据上下文来生成文本，这里的 context 可以做很多文章，对于生成文本的控制 (Seq2Seq 中的 Decoding 部分) 也可以做很多文章，anyway，利用 Seq2Seq 可以实现很多有趣的工作。</p>
<p>2.有哪些方法可以设计解码器的输出层？</p>
<p>答：最简单就是 RNN 每个隐层节点的输出 + Softmax</p>
<p>3.为什么 seq2seq 的 loss function 选择交叉熵损失函数？除了交叉熵损失函数，还可以有哪些选择？</p>
<p>答：seq2seq 在生成输出序列的时候是一个时刻生成一项，换句话说，在每个时间步他在解决一个分类问题。</p>
<p>而交叉熵损失函数是分类问题中最常用的损失函数，当然，如果 seq2seq 试图解决一个回归问题，我们可以使用别的损失函数，比如 L2 loss。</p>
<p>4.穷举搜索可否看做是特殊束宽的束搜索？为什么？</p>
<p>答：不能。穷举搜索在每个时间步并没有一个“束宽”来限制候选项的个数。</p>
<p>5.除了自然语言处理，注意力机制还可以应用在哪些地方？</p>
<p>答：CV, Speech and More。</p>
<p>Attention 机制的思想在于给模型引入了新的非线性操作。</p>
<p>传统 CNN 架构里非线性操作只有 max pooling 和非线性激活函数。虽说理论上只用这俩去拟合复杂函数仍然是可行的，但是会比较困难，且参数量巨大。相反，如果直接给一个与目标函数相近的非线性函数来作为函数线性组合的一个基，就方便很多了。</p>
<hr>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a href="http://zh.gluon.ai/chapter_natural-language-processing/seq2seq.html" target="_blank" rel="noopener">编码器-解码器，GLUON 动手学深度学习</a></li>
<li>MXNet/Gluon 中文频道，<a href="https://www.youtube.com/watch?v=GQh7wDQDc0Y" target="_blank" rel="noopener">动手学深度学习第十八课：seq2seq（编码器和解码器）和注意力机制</a></li>
<li>multiangle, <a href="http://blog.csdn.net/u014595019/article/details/52826423" target="_blank" rel="noopener">Encoder-Decoder模型和Attention模型</a></li>
<li>Goodfellow I, Bengio Y, Courville A, et al. Deep learning[M]. Cambridge: MIT press, 2016.</li>
<li>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</li>
<li>Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems (pp. 3104-3112).</li>
<li>Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate</li>
<li>Andrej Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks</li>
<li>Christopher Olah, Understanding LSTM Networks</li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>资磁一下？</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="https://raw.githubusercontent.com/yanshengjia/photo/master/wechatpay.jpg" alt="Shengjia Yan WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="https://raw.githubusercontent.com/yanshengjia/photo/master/alipay.jpg" alt="Shengjia Yan Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Seq2Seq/" rel="tag"># Seq2Seq</a>
          
            <a href="/tags/Attention/" rel="tag"># Attention</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/14/Introduction-to-LanguageTool/" rel="next" title="Introduction to LanguageTool">
                <i class="fa fa-chevron-left"></i> Introduction to LanguageTool
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- Go to www.addthis.com/dashboard to customize your tools -->
<div class="addthis_inline_share_toolbox">
  <script type = "text/javascript" src = "//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-589f4a3205db1792" async = "async" ></script>
</div>

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://raw.githubusercontent.com/yanshengjia/photo/master/skydive0.png"
               alt="Shengjia Yan" />
          <p class="site-author-name" itemprop="name">Shengjia Yan</p>
           
              <p class="site-description motion-element" itemprop="description">工程师 健身者 暴雪粉 | 人生不设限 | CS@SEU</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">32</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/yanshengjia" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.linkedin.com/in/yanshengjia" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/yanshengjia" target="_blank" title="Zhihu">
                  
                    <i class="fa fa-fw fa-book"></i>
                  
                  Zhihu
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/sjyan1995" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.instagram.com/yanshengjia/" target="_blank" title="Instagram">
                  
                    <i class="fa fa-fw fa-instagram"></i>
                  
                  Instagram
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.youtube.com/channel/UCHAyaP2nrMgOfIHtjKeyJyA" target="_blank" title="YouTube">
                  
                    <i class="fa fa-fw fa-youtube-play"></i>
                  
                  YouTube
                </a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://yanshengjia.com/leetcode" title="Conquer LeetCode" target="_blank">Conquer LeetCode</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://yanshengjia.com/lintcode" title="Conquer LintCode" target="_blank">Conquer LintCode</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wdxtub.com" title="小土刀" target="_blank">小土刀</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.shuang0420.com/" title="徐阿衡" target="_blank">徐阿衡</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.flickering.cn/" title="火光摇曳" target="_blank">火光摇曳</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.52nlp.cn/" title="52nlp" target="_blank">52nlp</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://nlp.stanford.edu/" title="Stanford NLP" target="_blank">Stanford NLP</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#介绍-Introduction"><span class="nav-number">1.</span> <span class="nav-text">介绍 Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#循环神经网络-RNN"><span class="nav-number">2.</span> <span class="nav-text">循环神经网络 RNN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#编码器-解码器-Encoder-Decoder"><span class="nav-number">3.</span> <span class="nav-text">编码器-解码器 Encoder-Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#编码器-Encoder"><span class="nav-number">3.1.</span> <span class="nav-text">编码器 Encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#解码器-Decoder"><span class="nav-number">3.2.</span> <span class="nav-text">解码器 Decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型训练-Model-Training"><span class="nav-number">3.3.</span> <span class="nav-text">模型训练 Model Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#训练技巧-Tricks"><span class="nav-number">3.3.1.</span> <span class="nav-text">训练技巧 Tricks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型预测-Model-Prediction"><span class="nav-number">3.4.</span> <span class="nav-text">模型预测 Model Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#穷举搜索-Exhaustive-Search"><span class="nav-number">3.4.1.</span> <span class="nav-text">穷举搜索 Exhaustive Search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#贪婪搜索-Greedy-Search"><span class="nav-number">3.4.2.</span> <span class="nav-text">贪婪搜索 Greedy Search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#束搜索-Beam-Search"><span class="nav-number">3.4.3.</span> <span class="nav-text">束搜索 Beam Search</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#注意力机制-Attention-Mechanism"><span class="nav-number">4.</span> <span class="nav-text">注意力机制 Attention Mechanism</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#动机-Motivation"><span class="nav-number">4.1.</span> <span class="nav-text">动机 Motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#设计-Design"><span class="nav-number">4.2.</span> <span class="nav-text">设计 Design</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#思考题"><span class="nav-number">5.</span> <span class="nav-text">思考题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">6.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shengjia Yan</span>
</div>

<!-- <div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div> -->

<div class="busuanzi-count">
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <span class="site-uv">
    <i class="fa fa-user" aria-hidden="true"></i>  <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
  </span>
  
  <span class="site-pv">
    <i class="fa fa-eye" aria-hidden="true"></i>  <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
  </span>
</div>


<div class="quote">
  守住初心
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>


        
      </div>
    </footer>

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://yanshengjia.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yanshengjia.com/2018/08/27/Seq2Seq-Encoder-Decoder-and-Attention-Mechanism/';
          this.page.identifier = '2018/08/27/Seq2Seq-Encoder-Decoder-and-Attention-Mechanism/';
          this.page.title = 'Seq2Seq (Encoder-Decoder) and Attention Mechanism';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://yanshengjia.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  





  






  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.1"></script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

</body>
</html>
